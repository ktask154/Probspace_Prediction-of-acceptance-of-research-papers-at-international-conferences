# ç ”ç©¶è«–æ–‡ã®å›½éš›å­¦ä¼šæ¡æŠäºˆæ¸¬  1st Place Solution ğŸ¥‡

[link](https://comp.probspace.com/competitions/paper_acception)

## Task
è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã©ã®è‡ªç„¶è¨€èªæƒ…å ±ã‹ã‚‰ã€ãã®è«–æ–‡ãŒå›½éš›å­¦ä¼šã§æ¡æŠã•ã‚Œã‚‹ã‹ã©ã†ã‹ã‚’äºˆæ¸¬

<br />

### è©•ä¾¡æŒ‡æ¨™
Accuracy

<br />

## ã‚·ãƒ³ã‚°ãƒ«ãƒ¢ãƒ‡ãƒ«
| exp | model | cv |
----- | ----- | --
 63 | deberta-v3-base | 0.7185
 64 | bert-base-uncased | 0.7207
 65 | deberta-base | 0.7203
 66 | roberta-base | 0.7195
 68 | bart-base | 0.7191
 69 | electra-base-discriminator | 0.7181
 70 | funnel-transformer/medium | 0.7189
 71 | albert-base-v2 | 0.7207
 72 | deberta-v3-small | 0.7193
 74 | distilbert-base-uncased | 0.7187
 75 | distilbart-cnn-12-6 | 0.7169
 76 | muppet-roberta-base | 0.7207
 77 | gpt-neo-125m | 0.7131
 
 
 <br />
 
ãƒ»input text : title + abstract <br />
ãƒ»cv : StratifiedKFold(n=10) <br />
ãƒ»Last Hidden State Output : Max Pooling <br />
ãƒ»epoch : 5 <br />
ãƒ»Loss : BCEWithLogitsLoss <br />
ãƒ»optimizer : AdamW <br />
ãƒ»LR : Layer-wise Learning Rate Decay (LLRD) <br />
ãƒ»æ•µå¯¾å­¦ç¿’æ–¹æ³•(AWP) <br />
ãƒ»MLM pretraining 10 epoch (probability=0.15) <br />

<br />

## ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
ãƒ»exp078 Stacking(CatBoost)

  01äºˆæ¸¬ã—ãŸã‚·ãƒ³ã‚°ãƒ«ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœ+ã‚¿ã‚¤ãƒˆãƒ«(ã‚¢ãƒ–ã‚¹ãƒˆ)ã®ãƒ¯ãƒ¼ãƒ‰æ•°+ã‚¿ã‚¤ãƒˆãƒ«(ã‚¢ãƒ–ã‚¹ãƒˆ)ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°+ã‚¿ã‚¤ãƒˆãƒ«(ã‚¢ãƒ–ã‚¹ãƒˆ)ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°
  
  CV : 0.7384
  
  Public : 0.7314
  
  Private : 0.7332
  
  

<br />

ãƒ»exp079 Voting

  13å€‹ã®ã‚·ãƒ³ã‚°ãƒ«ãƒ¢ãƒ‡ãƒ«ã®1äºˆæ¸¬ã•ã‚ŒãŸå€‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã€é–¾å€¤(=2)ã‚’ã‚’è¶…ãˆãŸã‚‚ã®ã‚’1äºˆæ¸¬
  
  CV : 0.7394
  
  Public : 0.7387
  
  Private : 0.7332
  
 
 
<br />
<br />
<br />

### ãƒ™ã‚¹ãƒˆã‚·ãƒ³ã‚°ãƒ«ãƒ¢ãƒ‡ãƒ«
| exp |model | cv | public | private |
----- | ---- | -- | ------ | -------
012 | deberta-v3-base | 0.7079 | 0.7397 | 0.7368

ãƒ»input text : abstract <br />
ãƒ»cv : StratifiedKFold(n=5) <br />
ãƒ»Last Hidden State Output : Mean Pooling <br />
ãƒ»Loss : BCELoss

